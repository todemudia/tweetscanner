# tweetscanner

# Bike Share Batch Analytics

# Project Background:
Local Governments are now on a quest of improving quality of life of its cities and towns by making them smarter. This quest towards smarter cities, involves creating a better sustainable urban environment for its citizens. The Bike Share Program was proposed to help cities become smarter, it would help relief traffic congestions, reduce air pollution in the city as well as having the side benefit of promoting a healthier lifestyle. The Bike Share program has been largely successfully in its objective as 500,000 bicycles are now available all around the world for people to use in over 500 different bike share programs.
In this report we will analyse one of these bike Share programs. Specifically, the dataset collected from a Bike share program in Washington DC, USA will be analysed. The dataset has 1000 records of rental data of bikes between the years of 2011-2013. The aim of this analysis is to understand the most popular rental season across the year.
# Middleware Configuration:
The configuration that has been selected for this report is the Hadoop standalone configuration. This configuration runs on a single node on the local machine. It is typically used for developing and debugging. Hadoop runs on completely on the local machine and does not use any Hadoop daemons, because of this it uses the local file system instead of HDFS. Hadoop Standalone is setup as soon as you have properly installed Hadoop. It is configured by leaving the configuration block of code in the xml documents empty.
The reason Hadoop standalone has been picked over Pseudo Distributed for this report is because of simplicity. In this report we are dealing with a small amount of data, it’s is better to use the local file system for such small data rather than HDFS. This is because the power of HDFS and Hadoop daemons would not be used to its full potential. It would be faster to use the local file system to process a small amount of data than the HDFS because the HDFS has to put the data in to the Hadoop File System before processing.
# Data Analytics Design:
The Bike Share data has data entries that will show the success of the Bike Share in various locations around the US. In order to calculate where the Bike Share program is having the most success, we would have to implement a Map Reduce Framework. Map Reduce is a program that splits huge amounts of data into smaller more easily processed chunks. Programs are executed in two major phases. The mapping phase and the reducing phase. However, before Mapping the splitting phase is implemented and before the reducing phase the Shuffling phase is also implemented. In the Mapping phase, the input data is fed into the mapper. In the reducing phase, all the outputs from the mapper is processed by the reduced and is set as the final result. The input in Map reduce programs are set as a list key Value pairs, the list is the broken up and each individual key value pair (K1, V1) is process by the mapper. The output of the mapper then aggregates into a larger (K2, V2) pairs. All the Keys that are the same are now paired into a new list of (K2, List(V2)) pairs. The reducer then processes each of the new key value pairs and aggregates their value into a new (K3, V3) pair. This new set of pairs is then written to file. See Fig.1. This map Reduce program could be used with any input file. For example, say you wanted to count the amount of times people have used the word “Brexit” in a data set gotten from a United Kingdom subreddit on the online social forum reddit. Provided the proper data is acquired MapReduce could find the number of times Brexit has been said in terabytes worth of data and more.
